{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHHqtpnzJaVu"
   },
   "source": "<a href=\"https://colab.research.google.com/github/abhi1021/resnet50-imagenet-1k/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2qR_1k5JaVu"
   },
   "source": "# Training ResNet50 on ImageNette2-160\n\n**Configuration:**\n- Model: ResNet50 (PyTorch)\n- Dataset: ImageNette2-160 (10 classes from ImageNet)\n- Epochs: 3\n- Batch Size: 128\n- Scheduler: OneCycle Learning Rate Policy"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD-egmL6JaVv"
   },
   "source": "## Clone Repository"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bauefBUAJaVv",
    "outputId": "586c7e75-04a1-4ab7-e6bd-c2ea0a0b75a7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "!git clone https://github.com/abhi1021/resnet50-imagenet-1k.git\n%cd resnet50-imagenet-1k"
  },
  {
   "cell_type": "markdown",
   "source": "## Download Dataset",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bnj1Ui1hJaVv"
   },
   "source": "# Download ImageNette2-160 dataset\nprint(\"Downloading ImageNette2-160 dataset...\")\n!wget -q https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\nprint(\"‚úì Download complete\")\n\nprint(\"\\nExtracting dataset...\")\n!tar -xzf imagenette2-160.tgz\nprint(\"‚úì Extraction complete\")\n\n# Count images\nimport os\ntrain_count = sum([len(files) for r, d, files in os.walk('imagenette2-160/train')])\nval_count = sum([len(files) for r, d, files in os.walk('imagenette2-160/val')])\nprint(f\"\\n‚úì Dataset ready:\")\nprint(f\"  Training images: {train_count}\")\nprint(f\"  Validation images: {val_count}\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WNIqyGmZJaVw"
   },
   "source": "!pip install -r requirements.txt -q"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "UVoGUJZ5JaVw",
    "outputId": "cdbd6c09-a1c8-4ab5-b108-533fdfa5d859",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "## Setup HuggingFace Token (Optional)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y4GHj8s8JaVx"
   },
   "source": "# Setup HuggingFace token (optional)\nHF_TOKEN = None\nHF_REPO_ID = None\n\ntry:\n    from google.colab import userdata\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    HF_REPO_ID = 'your-username/imagenette-resnet50'\n    print(\"‚úì HuggingFace token found\")\nexcept:\n    print(\"‚ÑπÔ∏è Training without HuggingFace upload\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQykX5GdJaVx"
   },
   "source": "## Start Training"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mxizp28cJaVx"
   },
   "source": "# Build training command\ncmd = \"python train.py --epochs 3 --batch-size 128 --model resnet50-pytorch --dataset imagenet --data-dir ./imagenette2-160 --num-classes 10 --scheduler onecycle\"\n\n# Add HuggingFace parameters if available\nif HF_TOKEN and HF_REPO_ID:\n    cmd += f\" --hf-token {HF_TOKEN} --hf-repo {HF_REPO_ID}\"\n\nprint(f\"Running: {cmd}\\n\")\nprint(\"=\"*70)\n\n# Run training\n!{cmd}"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDyI_JauJaVx"
   },
   "source": "## View Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-KGTBYLJaVx",
    "outputId": "ab9b7fb9-4d5e-4c72-9a8c-6f28f5f0217f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "outputs": [],
   "source": "import glob\nimport json\nimport os\n\n# Find the latest checkpoint directory\ncheckpoint_dirs = sorted(glob.glob('checkpoint_*'), reverse=True)\n\nif checkpoint_dirs:\n    latest_checkpoint = checkpoint_dirs[0]\n    print(f\"\\n{'='*70}\")\n    print(f\"LATEST CHECKPOINT: {latest_checkpoint}\")\n    print(f\"{'='*70}\\n\")\n\n    # Load and display metrics\n    metrics_file = os.path.join(latest_checkpoint, 'metrics.json')\n    if os.path.exists(metrics_file):\n        with open(metrics_file, 'r') as f:\n            metrics = json.load(f)\n\n        print(f\"üìä TRAINING RESULTS\")\n        print(f\"{'='*70}\")\n        print(f\"Best Test Accuracy: {metrics['best_test_accuracy']:.2f}%\")\n        print(f\"Best Epoch: {metrics['best_epoch']}\")\n        print(f\"Total Epochs Trained: {len(metrics['epochs'])}\")\n        print(f\"\\nFinal Metrics:\")\n        print(f\"  - Train Accuracy: {metrics['train_accuracies'][-1]:.2f}%\")\n        print(f\"  - Test Accuracy: {metrics['test_accuracies'][-1]:.2f}%\")\n        print(f\"  - Train Loss: {metrics['train_losses'][-1]:.4f}\")\n        print(f\"  - Test Loss: {metrics['test_losses'][-1]:.4f}\")\n        print(f\"{'='*70}\")\n\n    # List saved files\n    print(f\"\\nüìÅ Saved Files in {latest_checkpoint}:\")\n    for file in sorted(os.listdir(latest_checkpoint)):\n        file_path = os.path.join(latest_checkpoint, file)\n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n            print(f\"  - {file} ({file_size:.2f} MB)\")\n\n    print(f\"\\n{'='*70}\")\nelse:\n    print(\"‚ö† No checkpoint directories found.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlJmx9pTJaVy"
   },
   "source": "## Visualize Training Curves"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bdj5zvuTJaVy"
   },
   "source": "import matplotlib.pyplot as plt\nimport json\n\nif checkpoint_dirs:\n    latest_checkpoint = checkpoint_dirs[0]\n    metrics_file = os.path.join(latest_checkpoint, 'metrics.json')\n\n    if os.path.exists(metrics_file):\n        with open(metrics_file, 'r') as f:\n            metrics = json.load(f)\n\n        epochs = metrics['epochs']\n\n        # Create subplots\n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n        # Plot 1: Loss\n        axes[0, 0].plot(epochs, metrics['train_losses'], label='Train Loss', color='blue')\n        axes[0, 0].plot(epochs, metrics['test_losses'], label='Test Loss', color='red')\n        axes[0, 0].set_xlabel('Epoch')\n        axes[0, 0].set_ylabel('Loss')\n        axes[0, 0].set_title('Training and Test Loss')\n        axes[0, 0].legend()\n        axes[0, 0].grid(True, alpha=0.3)\n\n        # Plot 2: Accuracy\n        axes[0, 1].plot(epochs, metrics['train_accuracies'], label='Train Accuracy', color='blue')\n        axes[0, 1].plot(epochs, metrics['test_accuracies'], label='Test Accuracy', color='red')\n        axes[0, 1].set_xlabel('Epoch')\n        axes[0, 1].set_ylabel('Accuracy (%)')\n        axes[0, 1].set_title('Training and Test Accuracy')\n        axes[0, 1].legend()\n        axes[0, 1].grid(True, alpha=0.3)\n\n        # Plot 3: Learning Rate\n        axes[1, 0].plot(epochs, metrics['learning_rates'], color='green')\n        axes[1, 0].set_xlabel('Epoch')\n        axes[1, 0].set_ylabel('Learning Rate')\n        axes[1, 0].set_title('Learning Rate Schedule')\n        axes[1, 0].grid(True, alpha=0.3)\n\n        # Plot 4: Accuracy Gap\n        accuracy_gap = [train - test for train, test in zip(metrics['train_accuracies'], metrics['test_accuracies'])]\n        axes[1, 1].plot(epochs, accuracy_gap, color='orange')\n        axes[1, 1].set_xlabel('Epoch')\n        axes[1, 1].set_ylabel('Gap (%)')\n        axes[1, 1].set_title('Train-Test Accuracy Gap')\n        axes[1, 1].grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n\n        # Print summary statistics\n        print(f\"\\nüìä TRAINING SUMMARY\")\n        print(f\"{'='*70}\")\n        print(f\"Best Test Accuracy: {max(metrics['test_accuracies']):.2f}% (Epoch {metrics['test_accuracies'].index(max(metrics['test_accuracies'])) + 1})\")\n        print(f\"Final Test Accuracy: {metrics['test_accuracies'][-1]:.2f}%\")\n        print(f\"Final Train-Test Gap: {accuracy_gap[-1]:.2f}%\")\n        print(f\"Min Test Loss: {min(metrics['test_losses']):.4f}\")\n        print(f\"Max Learning Rate: {max(metrics['learning_rates']):.6f}\")\n        print(f\"Min Learning Rate: {min(metrics['learning_rates']):.6f}\")\n        print(f\"{'='*70}\")\n    else:\n        print(\"‚ö† Metrics file not found.\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Download Checkpoint",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from google.colab import files\nimport shutil\n\nif checkpoint_dirs:\n    latest_checkpoint = checkpoint_dirs[0]\n\n    # Create a zip file of the checkpoint directory\n    zip_filename = f\"{latest_checkpoint}.zip\"\n    shutil.make_archive(latest_checkpoint, 'zip', latest_checkpoint)\n\n    print(f\"‚úì Created {zip_filename}\")\n    print(f\"  Size: {os.path.getsize(zip_filename) / (1024*1024):.2f} MB\")\n    print(\"\\nDownloading...\")\n\n    # Download the zip file\n    files.download(zip_filename)\n\n    print(\"‚úì Download complete!\")\nelse:\n    print(\"‚ö† No checkpoint directories found.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}