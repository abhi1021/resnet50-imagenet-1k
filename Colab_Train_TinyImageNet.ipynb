{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Tiny ImageNet Training with ResNet18\n",
    "\n",
    "This notebook trains a ResNet18 model on the Tiny ImageNet dataset (200 classes, 64x64 images).\n",
    "\n",
    "**Requirements:**\n",
    "- Enable GPU runtime: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 recommended)\n",
    "- Google Drive for dataset storage (persists across sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-gpu-check"
   },
   "source": [
    "## 1. Environment Setup & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu-check"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úì GPU Runtime Enabled\")\n",
    "    print(f\"  Device: {gpu_name}\")\n",
    "    print(f\"  Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: No GPU detected!\")\n",
    "    print(\"   Training will be VERY slow on CPU.\")\n",
    "    print(\"   Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    print(\"\\n   Continue anyway? This may take several hours...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-drive"
   },
   "source": [
    "## 2. Mount Google Drive\n",
    "\n",
    "We'll store the dataset and trained models on Google Drive for persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories on Google Drive\n",
    "DRIVE_DATA_DIR = '/content/drive/MyDrive/tiny-imagenet-200'\n",
    "DRIVE_MODEL_DIR = '/content/drive/MyDrive/saved_model'\n",
    "\n",
    "os.makedirs(DRIVE_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Google Drive mounted\")\n",
    "print(f\"  Dataset location: {DRIVE_DATA_DIR}\")\n",
    "print(f\"  Model save location: {DRIVE_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-dataset"
   },
   "source": [
    "## 3. Download Tiny ImageNet Dataset\n",
    "\n",
    "Downloads the dataset to Google Drive if not already present (~237 MB compressed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-dataset"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_URL = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
    "DATASET_ZIP = '/content/drive/MyDrive/tiny-imagenet-200.zip'\n",
    "TRAIN_DIR = os.path.join(DRIVE_DATA_DIR, 'train')\n",
    "VAL_DIR = os.path.join(DRIVE_DATA_DIR, 'val')\n",
    "\n",
    "# Check if dataset already exists\n",
    "if os.path.exists(TRAIN_DIR) and os.path.exists(VAL_DIR):\n",
    "    num_train_classes = len([d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))])\n",
    "    num_val_images = len([f for f in Path(VAL_DIR).rglob('*.JPEG')])\n",
    "    \n",
    "    if num_train_classes == 200 and num_val_images > 0:\n",
    "        print(\"‚úì Dataset already exists on Google Drive\")\n",
    "        print(f\"  Train classes: {num_train_classes}\")\n",
    "        print(f\"  Val images: {num_val_images}\")\n",
    "        print(\"  Skipping download...\\n\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Dataset incomplete, re-downloading...\")\n",
    "        os.system(f'rm -rf {DRIVE_DATA_DIR}/*')\n",
    "        needs_download = True\n",
    "else:\n",
    "    print(\"Dataset not found. Downloading...\")\n",
    "    needs_download = True\n",
    "\n",
    "if 'needs_download' in locals() and needs_download:\n",
    "    print(f\"Downloading Tiny ImageNet from {DATASET_URL}\")\n",
    "    print(\"This may take 2-5 minutes...\\n\")\n",
    "    \n",
    "    # Download\n",
    "    !wget -q --show-progress {DATASET_URL} -O {DATASET_ZIP}\n",
    "    \n",
    "    print(\"\\nExtracting dataset to Google Drive...\")\n",
    "    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content/drive/MyDrive/')\n",
    "    \n",
    "    # Clean up zip file to save space\n",
    "    os.remove(DATASET_ZIP)\n",
    "    \n",
    "    print(\"‚úì Dataset downloaded and extracted\")\n",
    "    print(f\"  Location: {DRIVE_DATA_DIR}\")\n",
    "\n",
    "# Verify dataset structure\n",
    "print(\"\\nüìä Dataset Information:\")\n",
    "print(f\"  Train directory: {TRAIN_DIR}\")\n",
    "print(f\"  Val directory: {VAL_DIR}\")\n",
    "print(f\"  Number of classes: 200\")\n",
    "print(f\"  Image size: 64x64\")\n",
    "print(f\"  Train images per class: 500\")\n",
    "print(f\"  Validation images: 10,000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-clone"
   },
   "source": [
    "## 4. Clone Repository\n",
    "\n",
    "Clone the training code from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "REPO_URL = 'https://github.com/abhi1021/resnet50-imagenet-1k'\n",
    "REPO_DIR = '/content/resnet50-imagenet-1k'\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(\"Repository already cloned, pulling latest changes...\")\n",
    "    !cd {REPO_DIR} && git pull\n",
    "else:\n",
    "    print(f\"Cloning repository from {REPO_URL}...\")\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "\n",
    "# Change to repository directory\n",
    "%cd {REPO_DIR}\n",
    "\n",
    "print(f\"\\n‚úì Repository ready at {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-install"
   },
   "source": [
    "## 5. Install Dependencies\n",
    "\n",
    "Install required Python packages for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies from pyproject.toml\n",
    "print(\"Installing dependencies...\\n\")\n",
    "\n",
    "!pip install -q torch torchvision numpy matplotlib torchsummary tqdm albumentations grad-cam huggingface_hub\n",
    "\n",
    "print(\"\\n‚úì All dependencies installed\")\n",
    "\n",
    "# Verify installation\n",
    "import torch\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"  Albumentations version: {A.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-train"
   },
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Train ResNet18 on Tiny ImageNet with Colab-optimized parameters.\n",
    "\n",
    "**Training Parameters:**\n",
    "- Model: ResNet18 (200 classes)\n",
    "- Epochs: 20\n",
    "- Batch size: 256\n",
    "- Image size: 64x64\n",
    "- Optimizer: SGD (lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "**Expected training time:**\n",
    "- With GPU (T4): ~30-40 minutes\n",
    "- With CPU: ~8-12 hours (not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-model"
   },
   "outputs": [],
   "source": [
    "# Run training script with optimized parameters\n",
    "!python neural_network_analysis/train.py \\\n",
    "    --train-dir {TRAIN_DIR} \\\n",
    "    --val-dir {VAL_DIR} \\\n",
    "    --model-dir {DRIVE_MODEL_DIR} \\\n",
    "    --batch-size 256 \\\n",
    "    --img-size 64 \\\n",
    "    --num-workers 2 \\\n",
    "    --epochs 20 \\\n",
    "    --num-classes 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-results"
   },
   "source": [
    "## 7. Results & Trained Model\n",
    "\n",
    "The trained model has been saved to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-results"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"üìÅ Trained Model Location:\")\n",
    "print(f\"  {DRIVE_MODEL_DIR}\")\n",
    "print(\"\\nSaved files:\")\n",
    "\n",
    "if os.path.exists(DRIVE_MODEL_DIR):\n",
    "    for file in os.listdir(DRIVE_MODEL_DIR):\n",
    "        file_path = os.path.join(DRIVE_MODEL_DIR, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"  - {file} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(\"  No models found. Training may have failed.\")\n",
    "\n",
    "print(\"\\nüí° Tips:\")\n",
    "print(\"  - Model is saved on Google Drive and will persist\")\n",
    "print(\"  - You can download it from your Drive for local use\")\n",
    "print(\"  - To train for more epochs, adjust the --epochs parameter above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-optional"
   },
   "source": [
    "## Optional: Load and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Find the best model checkpoint\n",
    "model_files = [f for f in os.listdir(DRIVE_MODEL_DIR) if f.endswith('.pth')]\n",
    "\n",
    "if model_files:\n",
    "    # Load the checkpoint\n",
    "    best_model_path = os.path.join(DRIVE_MODEL_DIR, model_files[0])\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    \n",
    "    print(f\"‚úì Loaded model: {model_files[0]}\")\n",
    "    print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"  Best Accuracy: {checkpoint.get('best_acc', 'N/A'):.2f}%\")\n",
    "else:\n",
    "    print(\"No model checkpoints found.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
