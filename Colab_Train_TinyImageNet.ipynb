{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Tiny ImageNet Training with ResNet18\n",
    "\n",
    "This notebook trains a ResNet18 model on the Tiny ImageNet dataset (200 classes, 64x64 images).\n",
    "\n",
    "**Requirements:**\n",
    "- Enable GPU runtime: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 recommended)\n",
    "\n",
    "**Note:** Dataset and models are stored locally on the Colab VM and will be deleted when the runtime disconnects. Make sure to download your trained models before ending the session!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-gpu-check"
   },
   "source": [
    "## 1. Environment Setup & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu-check"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úì GPU Runtime Enabled\")\n",
    "    print(f\"  Device: {gpu_name}\")\n",
    "    print(f\"  Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: No GPU detected!\")\n",
    "    print(\"   Training will be VERY slow on CPU.\")\n",
    "    print(\"   Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    print(\"\\n   Continue anyway? This may take several hours...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-clone"
   },
   "source": "## 2. Clone Repository\n\nClone the training code from GitHub."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "REPO_URL = 'https://github.com/abhi1021/resnet50-imagenet-1k'\n",
    "REPO_DIR = '/content/resnet50-imagenet-1k'\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(\"Repository already cloned, pulling latest changes...\")\n",
    "    !cd {REPO_DIR} && git pull\n",
    "else:\n",
    "    print(f\"Cloning repository from {REPO_URL}...\")\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "\n",
    "# Change to repository directory\n",
    "%cd {REPO_DIR}\n",
    "\n",
    "print(f\"\\n‚úì Repository ready at {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-dataset"
   },
   "source": "## 4. Download Tiny ImageNet Dataset\n\nDownloads the dataset to local storage (~237 MB compressed, ~500 MB extracted).\n\n**Note:** The dataset will be deleted when the Colab runtime disconnects. It will need to be re-downloaded in new sessions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-dataset"
   },
   "outputs": [],
   "source": "import os\nimport zipfile\nfrom pathlib import Path\n\nDATASET_URL = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\nDATA_DIR = '/content/data'\nDATASET_ZIP = os.path.join(DATA_DIR, 'tiny-imagenet-200.zip')\nDATASET_DIR = os.path.join(DATA_DIR, 'tiny-imagenet-200')\nTRAIN_DIR = os.path.join(DATASET_DIR, 'train')\nVAL_DIR = os.path.join(DATASET_DIR, 'val')\n\n# Create data directory\nos.makedirs(DATA_DIR, exist_ok=True)\n\n# Check if dataset already exists\nif os.path.exists(TRAIN_DIR) and os.path.exists(VAL_DIR):\n    num_train_classes = len([d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))])\n    num_val_images = len([f for f in Path(VAL_DIR).rglob('*.JPEG')])\n    \n    if num_train_classes == 200 and num_val_images > 0:\n        print(\"‚úì Dataset already exists in local storage\")\n        print(f\"  Train classes: {num_train_classes}\")\n        print(f\"  Val images: {num_val_images}\")\n        print(\"  Skipping download...\\n\")\n    else:\n        print(\"‚ö†Ô∏è  Dataset incomplete, re-downloading...\")\n        os.system(f'rm -rf {DATASET_DIR}')\n        needs_download = True\nelse:\n    print(\"Dataset not found. Downloading...\")\n    needs_download = True\n\nif 'needs_download' in locals() and needs_download:\n    # Check if zip file already exists\n    if os.path.exists(DATASET_ZIP):\n        zip_size_mb = os.path.getsize(DATASET_ZIP) / (1024 * 1024)\n        print(f\"‚úì Zip file already exists: {DATASET_ZIP}\")\n        print(f\"  Size: {zip_size_mb:.1f} MB\")\n        print(\"  Skipping download, proceeding to extraction...\\n\")\n    else:\n        print(f\"Downloading Tiny ImageNet from {DATASET_URL}\")\n        print(\"This may take 2-5 minutes...\\n\")\n        \n        # Download\n        !wget -q --show-progress {DATASET_URL} -O {DATASET_ZIP}\n        print(\"\\n‚úì Download complete\")\n    \n    print(\"\\nExtracting dataset to local storage...\")\n    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:\n        zip_ref.extractall(DATA_DIR)\n    \n    # Clean up zip file to save space\n    os.remove(DATASET_ZIP)\n    \n    print(\"‚úì Dataset extracted and ready\")\n    print(f\"  Location: {DATASET_DIR}\")\n\n# Verify dataset structure\nprint(\"\\nüìä Dataset Information:\")\nprint(f\"  Train directory: {TRAIN_DIR}\")\nprint(f\"  Val directory: {VAL_DIR}\")\nprint(f\"  Number of classes: 200\")\nprint(f\"  Image size: 64x64\")\nprint(f\"  Train images per class: 500\")\nprint(f\"  Validation images: 10,000\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-install"
   },
   "source": "## 4. Install Dependencies\n\nInstall required Python packages for training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": "# Install dependencies from requirements.txt using pip\nprint(\"Installing dependencies from requirements.txt...\\n\")\n\n!pip install -r requirements.txt\n\nprint(\"\\n‚úì All dependencies installed\")\n\n# Verify installation\nimport torch\nimport torchvision\nimport albumentations as A\nprint(f\"  PyTorch version: {torch.__version__}\")\nprint(f\"  Torchvision version: {torchvision.__version__}\")\nprint(f\"  Albumentations version: {A.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-train"
   },
   "source": "## 5. Train the Model\n\nTrain ResNet18 on Tiny ImageNet with Colab-optimized parameters.\n\n**Training Parameters:**\n- Model: ResNet18 (200 classes)\n- Epochs: 20\n- Batch size: 256\n- Image size: 64x64\n- Optimizer: SGD (lr=0.1, momentum=0.9, weight_decay=5e-4)\n\n**Expected training time:**\n- With GPU (T4): ~30-40 minutes\n- With CPU: ~8-12 hours (not recommended)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-model"
   },
   "outputs": [],
   "source": "# Set model directory\nMODEL_DIR = '/content/saved_model'\nos.makedirs(MODEL_DIR, exist_ok=True)\n\n# Run training script with optimized parameters\n!python neural_network_analysis/train.py \\\n    --train-dir {TRAIN_DIR} \\\n    --val-dir {VAL_DIR} \\\n    --model-dir {MODEL_DIR} \\\n    --batch-size 256 \\\n    --img-size 64 \\\n    --num-workers 2 \\\n    --epochs 20 \\\n    --num-classes 200"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-results"
   },
   "source": "## 6. Results & Trained Model\n\nThe trained model has been saved to local storage.\n\n**‚ö†Ô∏è IMPORTANT:** Models are stored locally and will be deleted when the runtime disconnects. Download them now!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-results"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MODEL_DIR = '/content/saved_model'\n",
    "\n",
    "print(\"üìÅ Trained Model Location:\")\n",
    "print(f\"  {MODEL_DIR}\")\n",
    "print(\"\\nSaved files:\")\n",
    "\n",
    "if os.path.exists(MODEL_DIR):\n",
    "    for file in os.listdir(MODEL_DIR):\n",
    "        file_path = os.path.join(MODEL_DIR, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"  - {file} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(\"  No models found. Training may have failed.\")\n",
    "\n",
    "print(\"\\nüí° How to Download Models:\")\n",
    "print(\"  1. Click the folder icon on the left sidebar\")\n",
    "print(\"  2. Navigate to /content/saved_model/\")\n",
    "print(\"  3. Right-click on the .pth file and select 'Download'\")\n",
    "print(\"\\n‚ö†Ô∏è  WARNING: Models will be deleted when runtime disconnects!\")\n",
    "print(\"   Download them before ending your session.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-optional"
   },
   "source": "## Optional: Load and Test the Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "MODEL_DIR = '/content/saved_model'\n",
    "\n",
    "# Find the best model checkpoint\n",
    "model_files = [f for f in os.listdir(MODEL_DIR) if f.endswith('.pth')]\n",
    "\n",
    "if model_files:\n",
    "    # Load the checkpoint\n",
    "    best_model_path = os.path.join(MODEL_DIR, model_files[0])\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    \n",
    "    print(f\"‚úì Loaded model: {model_files[0]}\")\n",
    "    print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"  Best Accuracy: {checkpoint.get('best_acc', 'N/A'):.2f}%\")\n",
    "else:\n",
    "    print(\"No model checkpoints found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-download-helper"
   },
   "source": "## Optional: Download Model Using Code\n\nAlternative method to download the trained model programmatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-helper"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "MODEL_DIR = '/content/saved_model'\n",
    "\n",
    "# Download all .pth files\n",
    "model_files = [f for f in os.listdir(MODEL_DIR) if f.endswith('.pth')]\n",
    "\n",
    "if model_files:\n",
    "    print(\"Downloading trained models...\\n\")\n",
    "    for model_file in model_files:\n",
    "        model_path = os.path.join(MODEL_DIR, model_file)\n",
    "        print(f\"Downloading: {model_file}\")\n",
    "        files.download(model_path)\n",
    "    print(\"\\n‚úì All models downloaded!\")\n",
    "else:\n",
    "    print(\"No model files found to download.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}