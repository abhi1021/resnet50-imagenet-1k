{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc8404e",
   "metadata": {},
   "source": [
    "# Run training code in Colab\n",
    "\n",
    "This notebook recreates the `neural_network_analysis` package, downloads the Tiny-ImageNet dataset, and runs the modular training runner.\n",
    "\n",
    "Run cells top-to-bottom in Colab.\n",
    "\n",
    "If you want to use Google Drive for persistent storage, mount Drive and change the paths accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf379989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# Colab already provides a matching torch; we install utilities and albumentations\n",
    "!pip install -q albumentations==1.3.0 albumentations-pytorch torchsummary huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00abefe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage (optional)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Download and extract Tiny-ImageNet (v1.0) into /content/tiny-imagenet-200\n",
    "import os\n",
    "os.makedirs('/content/tiny-imagenet-200', exist_ok=True)\n",
    "# Download the Tiny-ImageNet zip (approx 250MB)\n",
    "!wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip -O /content/tiny-imagenet-200/tiny-imagenet-200.zip\n",
    "# Unzip the dataset\n",
    "!unzip -q /content/tiny-imagenet-200/tiny-imagenet-200.zip -d /content/tiny-imagenet-200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e0eb1",
   "metadata": {},
   "source": [
    "## Create the package files (data.py, model.py, trainer.py, utils.py, train.py) in Colab\n",
    "The notebook will write the same refactored modules into `/content/neural_network_analysis/` so the runner can import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbddf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_dir = '/content/neural_network_analysis'\n",
    "import os\n",
    "import textwrap\n",
    "os.makedirs(package_dir, exist_ok=True)\n",
    "\n",
    "# Write a minimal __init__.py\n",
    "with open(os.path.join(package_dir, '__init__.py'), 'w', encoding='utf-8') as f:\n",
    "    f.write('# Package init for neural_network_analysis\\n')\n",
    "\n",
    "# Cleanly define module sources using triple-double-quoted strings and textwrap.dedent\n",
    "files = {\n",
    "    'data.py': textwrap.dedent(\"\"\"\n",
    "        import os\n",
    "        import numpy as np\n",
    "        from torchvision import datasets, transforms\n",
    "        from torch.utils.data import DataLoader\n",
    "        from albumentations.pytorch import ToTensorV2\n",
    "        import albumentations as A\n",
    "\n",
    "\n",
    "        def get_transforms(img_size=64):\n",
    "            train_transforms = A.Compose([\n",
    "                A.RandomResizedCrop(size=(img_size, img_size), p=1.0),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "\n",
    "            def transform_wrapper(pil_img):\n",
    "                image_np = np.array(pil_img)\n",
    "                augmented = train_transforms(image=image_np)\n",
    "                return augmented['image']\n",
    "\n",
    "            test_transforms = transforms.Compose([\n",
    "                transforms.Resize(img_size),\n",
    "                transforms.CenterCrop(img_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "\n",
    "            return transform_wrapper, test_transforms\n",
    "\n",
    "\n",
    "        def get_dataloaders(train_dir, val_dir, batch_size=64, img_size=64, num_workers=0, pin_memory=False):\n",
    "            transform_train, transform_test = get_transforms(img_size=img_size)\n",
    "\n",
    "            train_ds = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "            val_ds = datasets.ImageFolder(val_dir, transform=transform_test)\n",
    "\n",
    "            dataloader_args = dict(shuffle=True, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
    "            train_loader = DataLoader(train_ds, **dataloader_args)\n",
    "            val_loader = DataLoader(val_ds, **dataloader_args)\n",
    "\n",
    "            return train_loader, val_loader, train_ds\n",
    "    \"\"\"),\n",
    "\n",
    "    'model.py': textwrap.dedent(\"\"\"\n",
    "        import torch.nn as nn\n",
    "        from torchvision import models\n",
    "\n",
    "\n",
    "        def build_resnet(model_name='resnet18', num_classes=200, pretrained=False):\n",
    "            \"\"\"Return a ResNet model with modified final layer.\"\"\"\n",
    "            model_map = {\n",
    "                'resnet18': models.resnet18,\n",
    "                'resnet34': models.resnet34,\n",
    "                'resnet50': models.resnet50,\n",
    "            }\n",
    "\n",
    "            if model_name not in model_map:\n",
    "                raise ValueError(f\"Unsupported model_name: {model_name}\")\n",
    "\n",
    "            model = model_map[model_name](weights=None if not pretrained else 'IMAGENET1K_V1')\n",
    "            in_features = model.fc.in_features\n",
    "            model.fc = nn.Linear(in_features, num_classes)\n",
    "            return model\n",
    "    \"\"\"),\n",
    "\n",
    "    'trainer.py': textwrap.dedent(\"\"\"\n",
    "        import torch\n",
    "        import numpy as np\n",
    "        import torch.nn.functional as F\n",
    "        from tqdm import tqdm\n",
    "        import os\n",
    "        from huggingface_hub import HfApi\n",
    "\n",
    "\n",
    "        def train_epoch(model, device, train_loader, optimizer, scheduler=None, cutmix_prob=0.0):\n",
    "            \"\"\"Run one training epoch.\n",
    "\n",
    "            Returns:\n",
    "                epoch_loss (float), epoch_acc (float), batch_losses (list), batch_accs (list)\n",
    "            \"\"\"\n",
    "            model.train()\n",
    "            correct = 0\n",
    "            processed = 0\n",
    "            batch_losses = []\n",
    "            batch_accs = []\n",
    "\n",
    "            pbar = tqdm(train_loader, desc='Train', leave=True)\n",
    "            for batch_idx, (data, target) in enumerate(pbar):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data)\n",
    "                loss = F.cross_entropy(outputs, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "                processed += data.size(0)\n",
    "                batch_losses.append(loss.item())\n",
    "\n",
    "                running_acc = 100. * correct / processed if processed > 0 else 0.0\n",
    "                batch_accs.append(running_acc)\n",
    "\n",
    "                # Mirror previous behavior: show loss, batch id, and running accuracy\n",
    "                pbar.set_description(desc=f'Loss={loss.item():.4f} Batch_id={batch_idx} Accuracy={running_acc:0.2f}')\n",
    "\n",
    "            epoch_loss = float(np.mean(batch_losses)) if batch_losses else 0.0\n",
    "            epoch_acc = 100. * correct / processed if processed > 0 else 0.0\n",
    "            return epoch_loss, epoch_acc, batch_losses, batch_accs\n",
    "\n",
    "\n",
    "        def evaluate(model, device, loader):\n",
    "            \"\"\"Evaluate model on `loader`. Returns (test_loss, acc, batch_losses, batch_accs).\"\"\"\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            batch_losses = []\n",
    "            batch_accs = []\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                pbar = tqdm(loader, desc='Eval', leave=True)\n",
    "                for batch_idx, (data, target) in enumerate(pbar):\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = F.cross_entropy(output, target, reduction='sum').item()\n",
    "                    test_loss += loss\n",
    "                    pred = output.argmax(dim=1)\n",
    "                    correct += pred.eq(target).sum().item()\n",
    "                    batch_size = data.size(0)\n",
    "                    total += batch_size\n",
    "                    batch_losses.append(loss / batch_size)\n",
    "                    batch_accs.append(100. * pred.eq(target).sum().item() / batch_size)\n",
    "                    pbar.set_description(desc=f'Eval Batch_id={batch_idx} Acc={100.*correct/total:0.2f}')\n",
    "\n",
    "            test_loss /= len(loader.dataset)\n",
    "            acc = 100. * correct / len(loader.dataset)\n",
    "            return test_loss, acc, batch_losses, batch_accs\n",
    "\n",
    "\n",
    "        def save_checkpoint_if_best(model, optimizer, epoch, test_acc, best_acc, model_dir, hf_username=None, model_name=None):\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                filename = os.path.join(model_dir, 'pytorch_model.pt')\n",
    "                torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'epoch': epoch, 'test_acc': test_acc}, filename)\n",
    "\n",
    "                # Optionally upload to Hugging Face hub if credentials/config provided\n",
    "                if hf_username and model_name and best_acc > 50.0:\n",
    "                    repo_id = f\"{hf_username}/{model_name}\"\n",
    "                    api = HfApi()\n",
    "                    try:\n",
    "                        api.upload_file(path_or_fileobj=filename, path_in_repo=os.path.basename(filename), repo_id=repo_id, repo_type='model')\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            return best_acc\n",
    "    \"\"\"),\n",
    "\n",
    "    'utils.py': textwrap.dedent(\"\"\"\n",
    "        import torch\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "\n",
    "\n",
    "        def get_device():\n",
    "            use_cuda = torch.cuda.is_available()\n",
    "            device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "            if use_cuda:\n",
    "                torch.backends.cudnn.benchmark = True\n",
    "            return device\n",
    "\n",
    "\n",
    "        def imshow_batch(img_batch, mean=None, std=None):\n",
    "            img = img_batch.clone().cpu()\n",
    "            npimg = img.numpy()\n",
    "            if mean is None:\n",
    "                mean = np.array([0.5071, 0.4867, 0.4408]).reshape(3,1,1)\n",
    "            if std is None:\n",
    "                std = np.array([0.2675, 0.2565, 0.2761]).reshape(3,1,1)\n",
    "            npimg = (npimg * std) + mean\n",
    "            npimg = np.transpose(npimg, (1, 2, 0))\n",
    "            plt.imshow(npimg)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    \"\"\"),\n",
    "\n",
    "    'train.py': textwrap.dedent(\"\"\"\n",
    "        \"\"\"Modular training runner for tiny-imagenet using neural_network_analysis modules.\n",
    "\n",
    "        This script is intentionally minimal — the heavy lifting lives in the\n",
    "        `neural_network_analysis` package (data.py, model.py, trainer.py, utils.py).\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import argparse\n",
    "        import torch.optim as optim\n",
    "\n",
    "        from neural_network_analysis import data, model as model_mod, trainer, utils\n",
    "\n",
    "\n",
    "        def parse_args():\n",
    "            p = argparse.ArgumentParser()\n",
    "            p.add_argument('--train-dir', default='./tiny-imagenet-200/train')\n",
    "            p.add_argument('--val-dir', default='./tiny-imagenet-200/val')\n",
    "            p.add_argument('--model-dir', default='./saved_model')\n",
    "            p.add_argument('--batch-size', type=int, default=128)\n",
    "            p.add_argument('--img-size', type=int, default=64)\n",
    "            p.add_argument('--num-workers', type=int, default=0)\n",
    "            p.add_argument('--epochs', type=int, default=30)\n",
    "            p.add_argument('--num-classes', type=int, default=200)\n",
    "            return p.parse_args()\n",
    "\n",
    "\n",
    "        def main():\n",
    "            args = parse_args()\n",
    "            train_dir = os.path.abspath(args.train_dir)\n",
    "            val_dir = os.path.abspath(args.val_dir)\n",
    "            model_dir = os.path.abspath(args.model_dir)\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "            device = utils.get_device()\n",
    "            pin_memory = (device.type == 'cuda')\n",
    "\n",
    "            train_loader, val_loader, train_ds = data.get_dataloaders(\n",
    "                train_dir, val_dir, batch_size=args.batch_size, img_size=args.img_size, num_workers=args.num_workers, pin_memory=pin_memory\n",
    "            )\n",
    "\n",
    "            # Visualize a small sample from the training loader using the helper in utils\n",
    "            try:\n",
    "                from torchvision import utils as tv_utils\n",
    "                dataiter = iter(train_loader)\n",
    "                imgs, labels = next(dataiter)\n",
    "                grid = tv_utils.make_grid(imgs[:4])\n",
    "                utils.imshow_batch(grid)\n",
    "            except Exception as e:\n",
    "                # Don't crash if visualization backend is not available (e.g., headless)\n",
    "                print('Batch visualization skipped:', e)\n",
    "\n",
    "            model = model_mod.build_resnet('resnet18', num_classes=args.num_classes, pretrained=False)\n",
    "            model = model.to(device)\n",
    "\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "            scheduler = None\n",
    "\n",
    "            best_acc = 0.0\n",
    "            # Metrics history (to match original script)\n",
    "            train_losses = []\n",
    "            test_losses = []\n",
    "            train_acc = []\n",
    "            test_acc = []\n",
    "            train_losses_epoch = []\n",
    "\n",
    "            for epoch in range(args.epochs):\n",
    "                print(f'EPOCH: {epoch}')\n",
    "                t_loss, t_acc, batch_losses, batch_accs = trainer.train_epoch(model, device, train_loader, optimizer, scheduler=scheduler)\n",
    "                v_loss, v_acc, v_batch_losses, v_batch_accs = trainer.evaluate(model, device, val_loader)\n",
    "\n",
    "                # Append per-batch data to global lists (original code appended per-batch for some lists)\n",
    "                train_losses.extend(batch_losses)\n",
    "                train_acc.extend(batch_accs)\n",
    "                test_losses.extend(v_batch_losses)\n",
    "                test_acc.extend(v_batch_accs)\n",
    "\n",
    "                # Epoch level\n",
    "                train_losses_epoch.append(t_loss)\n",
    "\n",
    "                print(f'Epoch {epoch}: Train loss {t_loss:.4f}, Train acc {t_acc:.2f}%, Val loss {v_loss:.4f}, Val acc {v_acc:.2f}%')\n",
    "\n",
    "                best_acc = trainer.save_checkpoint_if_best(model, optimizer, epoch, v_acc, best_acc, model_dir)\n",
    "\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "            main()\n",
    "    \"\"\")\n",
    "}\n",
    "\n",
    "# Write all files\n",
    "for name, src in files.items():\n",
    "    path = os.path.join(package_dir, name)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(src)\n",
    "\n",
    "print('Wrote neural_network_analysis package to', package_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37acd1e6",
   "metadata": {},
   "source": [
    "## Run the training runner (quick smoke test)\n",
    "The command below runs the modular runner against Imagenette extracted under /content/imagenette/imagenette2-160. This is a smoke test and may take time depending on epochs and hardware.\n",
    "Adjust `--batch-size` and `--epochs` for Colab (e.g., batch-size=64 epochs=1 for a quick run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44155f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the modular training script against Tiny-ImageNet (quick run)\n",
    "!python -m neural_network_analysis.train --train-dir /content/tiny-imagenet-200/train --val-dir /content/tiny-imagenet-200/val --batch-size 64 --epochs 1 --model-dir /content/drive/MyDrive/saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63014943",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- If you want to persist the saved model to your Google Drive, mount Drive and set the `--model-dir` to a folder under `/content/drive/MyDrive/`.\n",
    "- If Colab runs out of RAM, lower `--batch-size` and `--num-workers`.\n",
    "- If you prefer Tiny-ImageNet instead of Imagenette, replace the dataset download cell with a Tiny-ImageNet download (dataset is ~500MB).\n",
    "\n",
    "If you want, I can also: add an explicit Drive-mount cell, or create a small `requirements.txt` cell to pin versions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
